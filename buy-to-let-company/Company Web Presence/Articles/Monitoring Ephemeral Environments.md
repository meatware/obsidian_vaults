**Monitoring system suitable for a microservices architecture**

The use of cloud services and containers creates issues for traditional host-centric monitoring systems. Containers allow scalability and are commonly used in microservice architectures to run various services. This approach allows different services to be easily swapped out and modified as required. The technology also allows escape from dependency hell by packaging an entire mini-host in an isolated, lightweight virtual runtime. However, the use of multiple containers often causes a massive increase in the number of available metrics. This can cause logistical problems and the resulting deluge of information can be very difficult to decipher and understand. This problem is exacerbated by the often ephemeral natures of containers which can have lifetimes measured in minutes or hours. Monitoring these short-lived systems requires a large increase in metric sampling frequency. 

**Monitoring system requirements:**

1\. Produces a digestible amount of metrics.

2\. Make correlations between disparate metrics for different services (Sophisticated visualisation tools).

3\. Handles dependencies between applications. E.g., minimise frontend alert messages if a shared database goes down. 

3\. Ability to share metrics within the team so solutions can be quickly implemented.

4\. Ability to search and compile views on the fly to hone in on potential issues.

5\. Produce network graphs that model interconnectivity of services, e.g., simianviz.

6\. Use of historical data and anomaly prediction (e.g, Skyline) versus checking if parameters have exceeded alert thresholds.

7\. Given epheremal nature of network it would be better if data is pushed to the monitoring backend.

8\. Fine granularity of ~20 seconds rules out many established monitoring systems like Nagios, Zabbix, Cacti, etc. 

Some of the metrics that could be measured in a hypothetical monitoring system are listed below:

**A) Monitoring Docker containers:**

Instead of monitoring each container like a host, monitor all the layers in the entire stack: Application Performance Monitoring, infrastructure monitoring and hardware monitoring (Fig.1). Visualising the data simultaneously allows the determination of how problems in one part of the stack affect others.

![](Aspose.Words.2ae42224-bd99-48b1-bca6-67b211dfecde.001.png "docker_p1_6")

***Fig.1.** Effective monitoring of stack layers.* 

Docker (container specific) statistics:

\* User and system CPU (percent of time).

\* Throttling count and time.

\* Memory. E.g., cache, swap, etc.

\* I/O statistics.

\* Network. E.g., traffic volume and errors in transmission.

Can collect metrics using:

1\. Pseudo-files mounted in sysfs (fastest and most lightweight method).

2\. Docker stats command (live stream basic metrics).

3\. API produces live stream of very detailed configurable metrics.

\* Docker Daemon listens on /var/run/docker.sock.

\* can use netcat (UDP or TCP) to make API calls to socket.

\* Can collect all metrics as a continuously updated live stream of JSON.

Monitoring backend receives information from an agent that runs within a container or natively on the host. 

**B) Monitoring custom application metrics**

Use StatsD to collect application specific metrics via UDP. This is a demon developed by Etsy that relays aggregate metrics to any monitoring backend. Developers insert code that collects data via metrics such as:

\* Gauges: number of users connected.

\* Counters: Number of times something occurs over a period of time.

\* Histograms: Statistical distributions.

The StatsD demon listens to the UDP traffic generated by all the applications and flushes the data at defined intervals to the monitoring backend. To cut down on sending too many UDP packets with events that occur frequently, StatsD allows an option to sparsely sample data, e.g., record 1 in every 10 events. 

UDP is a fire-and-forget protocol, so the application is not slowed down by waiting for message received confirmations. However, this means that data could be lost as errors in transmission/receipt are not identified. Also note that UDP is susceptible to spoofing attacks. 

StatsD is compatible with many languages: Ruby, Python, Java, Erlang, Scala, Haskell, etc.

**C) Monitoring cluster metrics with Amazon CloudWatch**

Measure metrics like CPU, Network, DiskReadBytes and DiskWriteBytes. Can also monitor Amazon database services.

**D) Monitoring: Graphite with Grafana frontend**

The monitoring backend chosen is graphite. It eases the visualisation of metrics and new monitored objects can be created on-the-fly. This reduces the time spent pre-configuring newly implemented metrics. It can also deal with highly granular UDP driven data input from demons such as statsD and netcat. There are many more metric gatherers that support the following graphite metric protocols: plaintext, pickle and AMPQ.

![](Aspose.Words.2ae42224-bd99-48b1-bca6-67b211dfecde.002.png "graphite-architecture")

***Fig.2.** Graphite architecture showing 3 main parts of graphite.*

Graphite has 3 main components which can be mixed and matched with alternatives depending on needs and taste:

1\. Graphite-web: Django-based front end for management and visualisation.

\* Visualisation is lacking.

\* Grafana and other frontend alternatives are superior.

\* Plugin architecture can extend functionality to monitor SQL databases.

\* Grafana supports InfluxDB and Amazon CloudWatch.

2\. Carbon: Demons that ingests stats from network and writes to disk.

\* Can buffer metrics over a period of time and write to disk.

\* Problems when system is scaled up and multiples threads not handled properly. Results in dropped metrics.

\* Does not maintain open file handles to database, so multiple read/write penalty incurred. 

\* graphite-ng is a Carbon alternative written in Go that ameliorates the issues mentioned above.

3\. Whisper: Database replacement for RRDTool (See below).

Systems built around RRDTool have been prevalent for many years. Data is stored in a circular "round-robin" database. The amount of data stored in the database is predefined and constant. 

Monitoring systems such as graphite use a fixed size RRD known as Whisper which can handle irregularly occurring data entries. However, it is much slower and can be storage-inefficient. Lot of IO calls are made and scalability is known to be poor. InfluxDB has been written in Go to address these scalability issues. 


**Conclusion**

Is this the perfect monitoring system? No, it probably would do 5-10 years ago, but does not fully answer the challenges presented by a microservices architecture. Newer monitoring systems are more intelligent and can make better correlations between different data sources. Furthermore, monitoring solution packages such as AppDynamics and Dynatrace allow:

1\. End user experience monitoring:

\* Browser Real-User : JavaScript injection approach that capture metrics related to user requests and sessions across different browsers and devices, e.g., collate specific user actions within an app to get a history and timings to understand potentials performance issues.

\* Synthetic monitoring: Monitors host availability and latencies from different parts of the world 24/7. Can also simulate high volume traffic and test executions from different localities.

2\. Live topology maps:

\* Autodiscovery of shifting microservice architecture.

\* Easy visualisation of service dependencies.

\* Identify bottlenecks in the system.

Out-of-the-box solutions offered by AppDynamics, Datadog, Dynatrace Ruxit would be very good monitoring solutions for microservice architectures.



**Appendix**

**Notes on other graphing/visualisation methods**

\1) Matplotlib: 

Generates publication quality images - but slow (10 frames per second).

Can get faster frame rate by using a different backend.

\* Interactive backend (use in pygtk, wxpython, qt4, etc).

\* Hardcopy backends (PNG, SVG, PDF, PS, etc).

Can increase speed by:

\* Using blitting to avoid redraw things that don't change e.g. axes, background, subplots, legend, etc. - Requires backend specific code.

\* Using correct backend. 

GUI-neutral example gets 200 frames per second.

GUI-specific (e.g. GTKAgg) will be much faster.

\2) PyQwt is very fast, but you have to make a GUI with QT to run it, which might not be the best solution for the monitoring system\. Outputting the data to a web site might be a better solution\.

\3) Bokeh is an interactive visualisation library\.

\* Make data driven visualisations.

\* Can create animated plots and stream realtime data. 

\* Can interact by using zoom, pan & resize.

\* Offers a light-weight webserver that can display plots in your browser.

\* Plotting commands are in python.

\* Data is created in Python and then converted in JSON to be used by a javascript library that renders visuals in browser. Therefore Client processes the rendering.

\* Possibly not that great for on-the-fly metrics.

\4) Use inbuilt visualisation tools from an out-of-the-box solution such as Datadog\.

\5) Use graphite with graphana replacement\.